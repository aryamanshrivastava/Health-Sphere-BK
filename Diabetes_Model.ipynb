{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Features:\n",
      "Index(['pedi', 'mass', 'preg', 'age', 'plas'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Load the dataset\n",
    "url = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv\"\n",
    "names = ['preg', 'plas', 'pres', 'skin', 'insu', 'mass', 'pedi', 'age', 'class']\n",
    "data = pd.read_csv(url, names=names)\n",
    "\n",
    "# Split data into features and target variable\n",
    "X = data.drop('class', axis=1)\n",
    "y = data['class']\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train logistic regression model\n",
    "lr_model = LogisticRegression(max_iter=1000)\n",
    "lr_model.fit(X_train, y_train)\n",
    "\n",
    "# Get coefficients (importance) of features\n",
    "coefficients = np.abs(lr_model.coef_[0])\n",
    "\n",
    "# Rank features based on coefficients\n",
    "feature_ranking = np.argsort(coefficients)[::-1]\n",
    "\n",
    "# Select top N features\n",
    "num_features_to_select = 5\n",
    "selected_features = X.columns[feature_ranking][:num_features_to_select]\n",
    "\n",
    "# Display selected features\n",
    "print(\"Selected Features:\")\n",
    "print(selected_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['pedi', 'mass', 'preg', 'age', 'plas'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(f'{selected_features}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.8192182410423453\n",
      "Test Accuracy: 0.7662337662337663\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Selecting features from the dataset\n",
    "X_train_selected = X_train[selected_features]\n",
    "X_test_selected = X_test[selected_features]\n",
    "\n",
    "# Train KNN model\n",
    "knn_model = KNeighborsClassifier(n_neighbors=5)  # You can choose the value of k as desired\n",
    "knn_model.fit(X_train_selected, y_train)\n",
    "\n",
    "# Evaluate model\n",
    "train_accuracy = knn_model.score(X_train_selected, y_train)\n",
    "test_accuracy = knn_model.score(X_test_selected, y_test)\n",
    "\n",
    "print(\"Train Accuracy:\", train_accuracy)\n",
    "print(\"Test Accuracy:\", test_accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Train Accuracy: 0.7687296416938111\n",
      "SVM Test Accuracy: 0.7597402597402597\n",
      "Naive Bayes Train Accuracy: 0.755700325732899\n",
      "Naive Bayes Test Accuracy: 0.7532467532467533\n",
      "Random Forest Train Accuracy: 1.0\n",
      "Random Forest Test Accuracy: 0.7662337662337663\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Train SVM model\n",
    "svm_model = SVC(kernel='linear')  # You can choose different kernels (e.g., 'rbf') and parameters as desired\n",
    "svm_model.fit(X_train_selected, y_train)\n",
    "\n",
    "# Train Naive Bayes model\n",
    "nb_model = GaussianNB()\n",
    "nb_model.fit(X_train_selected, y_train)\n",
    "\n",
    "# Train Random Forest model\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)  # You can adjust the number of trees (n_estimators) as desired\n",
    "rf_model.fit(X_train_selected, y_train)\n",
    "\n",
    "# Evaluate models\n",
    "svm_train_accuracy = svm_model.score(X_train_selected, y_train)\n",
    "svm_test_accuracy = svm_model.score(X_test_selected, y_test)\n",
    "\n",
    "nb_train_accuracy = nb_model.score(X_train_selected, y_train)\n",
    "nb_test_accuracy = nb_model.score(X_test_selected, y_test)\n",
    "\n",
    "rf_train_accuracy = rf_model.score(X_train_selected, y_train)\n",
    "rf_test_accuracy = rf_model.score(X_test_selected, y_test)\n",
    "\n",
    "# Print accuracies\n",
    "print(\"SVM Train Accuracy:\", svm_train_accuracy)\n",
    "print(\"SVM Test Accuracy:\", svm_test_accuracy)\n",
    "print(\"Naive Bayes Train Accuracy:\", nb_train_accuracy)\n",
    "print(\"Naive Bayes Test Accuracy:\", nb_test_accuracy)\n",
    "print(\"Random Forest Train Accuracy:\", rf_train_accuracy)\n",
    "print(\"Random Forest Test Accuracy:\", rf_test_accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Boosting Train Accuracy: 0.9055374592833876\n",
      "Gradient Boosting Test Accuracy: 0.7207792207792207\n",
      "Best Parameters: {'learning_rate': 0.1, 'max_depth': 3, 'min_samples_leaf': 4, 'min_samples_split': 10, 'n_estimators': 100}\n",
      "Train Accuracy: 0.9006514657980456\n",
      "Test Accuracy: 0.7467532467532467\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "# Train Gradient Boosting model\n",
    "gbm_model = GradientBoostingClassifier()\n",
    "gbm_model.fit(X_train_selected, y_train)\n",
    "\n",
    "# Evaluate model\n",
    "gbm_train_accuracy = gbm_model.score(X_train_selected, y_train)\n",
    "gbm_test_accuracy = gbm_model.score(X_test_selected, y_test)\n",
    "\n",
    "print(\"Gradient Boosting Train Accuracy:\", gbm_train_accuracy)\n",
    "print(\"Gradient Boosting Test Accuracy:\", gbm_test_accuracy)\n",
    "\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define the Gradient Boosting Classifier\n",
    "gbm_model = GradientBoostingClassifier()\n",
    "\n",
    "# Define the hyperparameters grid\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],  # Number of boosting stages\n",
    "    'learning_rate': [0.1, 0.01, 0.001],  # Step size shrinkage used to prevent overfitting\n",
    "    'max_depth': [3, 5, 7],  # Maximum depth of the individual estimators\n",
    "    'min_samples_split': [2, 5, 10],  # Minimum number of samples required to split an internal node\n",
    "    'min_samples_leaf': [1, 2, 4]  # Minimum number of samples required to be at a leaf node\n",
    "}\n",
    "\n",
    "# Perform GridSearchCV\n",
    "grid_search = GridSearchCV(gbm_model, param_grid, cv=5, scoring='accuracy')\n",
    "grid_search.fit(X_train_selected, y_train)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_params = grid_search.best_params_\n",
    "\n",
    "# Train the model with the best hyperparameters\n",
    "best_gbm_model = GradientBoostingClassifier(**best_params)\n",
    "best_gbm_model.fit(X_train_selected, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "train_accuracy = best_gbm_model.score(X_train_selected, y_train)\n",
    "test_accuracy = best_gbm_model.score(X_test_selected, y_test)\n",
    "\n",
    "print(\"Best Parameters:\", best_params)\n",
    "print(\"Train Accuracy:\", train_accuracy)\n",
    "print(\"Test Accuracy:\", test_accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoost Train Accuracy: 0.8322475570032574\n",
      "AdaBoost Test Accuracy: 0.7597402597402597\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "# Train AdaBoost model\n",
    "adaboost_model = AdaBoostClassifier()\n",
    "adaboost_model.fit(X_train_selected, y_train)\n",
    "\n",
    "# Evaluate model\n",
    "adaboost_train_accuracy = adaboost_model.score(X_train_selected, y_train)\n",
    "adaboost_test_accuracy = adaboost_model.score(X_test_selected, y_test)\n",
    "\n",
    "print(\"AdaBoost Train Accuracy:\", adaboost_train_accuracy)\n",
    "print(\"AdaBoost Test Accuracy:\", adaboost_test_accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost Train Accuracy: 1.0\n",
      "XGBoost Test Accuracy: 0.7402597402597403\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "# Train XGBoost model\n",
    "xgb_model = xgb.XGBClassifier()\n",
    "xgb_model.fit(X_train_selected, y_train)\n",
    "\n",
    "# Evaluate model\n",
    "xgb_train_accuracy = xgb_model.score(X_train_selected, y_train)\n",
    "xgb_test_accuracy = xgb_model.score(X_test_selected, y_test)\n",
    "\n",
    "print(\"XGBoost Train Accuracy:\", xgb_train_accuracy)\n",
    "print(\"XGBoost Test Accuracy:\", xgb_test_accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\AVIK PRASAD\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\AVIK PRASAD\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\AVIK PRASAD\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\AVIK PRASAD\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\AVIK PRASAD\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\AVIK PRASAD\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\AVIK PRASAD\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\AVIK PRASAD\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\AVIK PRASAD\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\AVIK PRASAD\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\AVIK PRASAD\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\AVIK PRASAD\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\AVIK PRASAD\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\AVIK PRASAD\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\AVIK PRASAD\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\AVIK PRASAD\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\AVIK PRASAD\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\AVIK PRASAD\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\AVIK PRASAD\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\AVIK PRASAD\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\AVIK PRASAD\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\AVIK PRASAD\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\AVIK PRASAD\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\AVIK PRASAD\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\AVIK PRASAD\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\AVIK PRASAD\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\AVIK PRASAD\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\AVIK PRASAD\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'C': 1, 'solver': 'lbfgs'}\n",
      "Train Accuracy: 0.7638436482084691\n",
      "Test Accuracy: 0.7532467532467533\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\AVIK PRASAD\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\AVIK PRASAD\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define Logistic Regression model\n",
    "lr_model = LogisticRegression(max_iter=1000)\n",
    "\n",
    "# Define hyperparameters grid\n",
    "param_grid = {\n",
    "    'C': [0.001, 0.01, 0.1, 1, 10, 100],  # Regularization parameter\n",
    "    'solver': ['liblinear', 'lbfgs', 'saga']  # Algorithm to use in the optimization problem\n",
    "}\n",
    "\n",
    "# Perform GridSearchCV\n",
    "grid_search = GridSearchCV(lr_model, param_grid, cv=5, scoring='accuracy')\n",
    "grid_search.fit(X_train_selected, y_train)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_params = grid_search.best_params_\n",
    "\n",
    "# Train the model with the best hyperparameters\n",
    "best_lr_model = LogisticRegression(**best_params, max_iter=1000)\n",
    "best_lr_model.fit(X_train_selected, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "train_accuracy = best_lr_model.score(X_train_selected, y_train)\n",
    "test_accuracy = best_lr_model.score(X_test_selected, y_test)\n",
    "\n",
    "print(\"Best Parameters:\", best_params)\n",
    "print(\"Train Accuracy:\", train_accuracy)\n",
    "print(\"Test Accuracy:\", test_accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'n_neighbors': 9, 'p': 1, 'weights': 'uniform'}\n",
      "Train Accuracy: 0.8029315960912052\n",
      "Test Accuracy: 0.7532467532467533\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define KNN model\n",
    "knn_model = KNeighborsClassifier()\n",
    "\n",
    "# Define hyperparameters grid\n",
    "param_grid = {\n",
    "    'n_neighbors': [3, 5, 7, 9, 11, 13],  # Number of neighbors\n",
    "    'weights': ['uniform', 'distance'],  # Weight function used in prediction\n",
    "    'p': [1, 2]  # Power parameter for the Minkowski metric\n",
    "}\n",
    "\n",
    "# Perform GridSearchCV\n",
    "grid_search = GridSearchCV(knn_model, param_grid, cv=5, scoring='accuracy')\n",
    "grid_search.fit(X_train_selected, y_train)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_params = grid_search.best_params_\n",
    "\n",
    "# Train the model with the best hyperparameters\n",
    "best_knn_model = KNeighborsClassifier(**best_params)\n",
    "best_knn_model.fit(X_train_selected, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "train_accuracy = best_knn_model.score(X_train_selected, y_train)\n",
    "test_accuracy = best_knn_model.score(X_test_selected, y_test)\n",
    "\n",
    "print(\"Best Parameters:\", best_params)\n",
    "print(\"Train Accuracy:\", train_accuracy)\n",
    "print(\"Test Accuracy:\", test_accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'max_depth': 10, 'min_samples_leaf': 1, 'min_samples_split': 10, 'n_estimators': 200}\n",
      "Train Accuracy: 0.9299674267100977\n",
      "Test Accuracy: 0.7402597402597403\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define the Random Forest Classifier\n",
    "rf_model = RandomForestClassifier()\n",
    "\n",
    "# Define the hyperparameters grid\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],  # Number of trees in the forest\n",
    "    'max_depth': [None, 10, 20],  # Maximum depth of the trees\n",
    "    'min_samples_split': [2, 5, 10],  # Minimum number of samples required to split an internal node\n",
    "    'min_samples_leaf': [1, 2, 4]  # Minimum number of samples required to be at a leaf node\n",
    "}\n",
    "\n",
    "# Perform GridSearchCV\n",
    "grid_search = GridSearchCV(rf_model, param_grid, cv=5, scoring='accuracy')\n",
    "grid_search.fit(X_train_selected, y_train)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_params = grid_search.best_params_\n",
    "\n",
    "\n",
    "# Train the model with the best hyperparameters\n",
    "best_rf_model = RandomForestClassifier(**best_params)\n",
    "best_rf_model.fit(X_train_selected, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "train_accuracy = best_rf_model.score(X_train_selected, y_train)\n",
    "test_accuracy = best_rf_model.score(X_test_selected, y_test)\n",
    "\n",
    "print(\"Best Parameters:\", best_params)\n",
    "print(\"Train Accuracy:\", train_accuracy)\n",
    "print(\"Test Accuracy:\", test_accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'max_depth': 5, 'min_samples_leaf': 4, 'min_samples_split': 2}\n",
      "Train Accuracy: 0.8241042345276873\n",
      "Test Accuracy: 0.7727272727272727\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define the Decision Tree Classifier\n",
    "dt_model = DecisionTreeClassifier()\n",
    "\n",
    "# Define the hyperparameters grid\n",
    "param_grid = {\n",
    "    'max_depth': [None, 5, 10, 20],  # Maximum depth of the tree\n",
    "    'min_samples_split': [2, 5, 10],  # Minimum number of samples required to split an internal node\n",
    "    'min_samples_leaf': [1, 2, 4]  # Minimum number of samples required to be at a leaf node\n",
    "}\n",
    "\n",
    "# Perform GridSearchCV\n",
    "grid_search = GridSearchCV(dt_model, param_grid, cv=5, scoring='accuracy')\n",
    "grid_search.fit(X_train_selected, y_train)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_params = grid_search.best_params_\n",
    "\n",
    "# Train the model with the best hyperparameters\n",
    "best_dt_model = DecisionTreeClassifier(**best_params)\n",
    "best_dt_model.fit(X_train_selected, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "train_accuracy = best_dt_model.score(X_train_selected, y_train)\n",
    "test_accuracy = best_dt_model.score(X_test_selected, y_test)\n",
    "\n",
    "print(\"Best Parameters:\", best_params)\n",
    "print(\"Train Accuracy:\", train_accuracy)\n",
    "print(\"Test Accuracy:\", test_accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['diabetes_model.joblib']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from joblib import dump, load\n",
    "\n",
    "dump(best_rf_model, 'diabetes_model.joblib')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Person is not diabetic\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\AVIK PRASAD\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but RandomForestClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "loaded_model = load('diabetes_model.joblib')\n",
    "\n",
    "input_values = ['9', '102', '32.9', '0.665', '46']\n",
    "\n",
    "input_array = np.array(input_values).reshape(1, -1)\n",
    "\n",
    "prediction = loaded_model.predict(input_array)\n",
    "\n",
    "if prediction[0] == 0:\n",
    "    print(\"Person is not diabetic\")\n",
    "else:\n",
    "    print(\"Person is diabetic\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0.665', '32.9', '9', '46', '102']\n",
      "[1]\n",
      "Person is diabetic\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\AVIK PRASAD\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but RandomForestClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "json = {\n",
    "  \"pregnancies\": \"9\",\n",
    "  \"glucose\": \"102\",\n",
    "  \"bloodPressure\": \"76\",\n",
    "  \"skinThickness\": \"97\",\n",
    "  \"insulin\": \"0\",\n",
    "  \"bmi\": \"32.9\",\n",
    "  \"diabetesPedigree\": \"0.665\",\n",
    "  \"age\": \"46\"\n",
    "}\n",
    "\n",
    "elements_to_consider = ['diabetesPedigree', 'bmi', 'pregnancies','age', 'glucose']\n",
    "input_list = []\n",
    "\n",
    "for element in elements_to_consider:\n",
    "    if element in json:\n",
    "        input_list.append(json.pop(element))\n",
    "print(input_list)\n",
    "\n",
    "loaded_model = load('diabetes_model.joblib')\n",
    "\n",
    "input_array = np.array(input_list).reshape(1, -1)\n",
    "\n",
    "prediction = loaded_model.predict(input_array)\n",
    "\n",
    "if prediction[0] == 0:\n",
    "    print(prediction)\n",
    "    print(\"Person is not diabetic\")\n",
    "else:\n",
    "    print(prediction)\n",
    "    print(\"Person is diabetic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
